{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def load_and_explore_datasets(folder_path):\n",
    "    train_file = os.path.join(folder_path, \"train.csv\")\n",
    "    val_file = os.path.join(folder_path, \"val.csv\")\n",
    "    test_file = os.path.join(folder_path, \"test.csv\")\n",
    "\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    val_data = pd.read_csv(val_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "\n",
    "    print(\"Train Dataset:\")\n",
    "    print(train_data.info())\n",
    "    print(train_data.head(), \"\\n\")\n",
    "\n",
    "    print(\"Validation Dataset:\")\n",
    "    print(val_data.info())\n",
    "    print(val_data.head(), \"\\n\")\n",
    "\n",
    "    print(\"Test Dataset:\")\n",
    "    print(test_data.info())\n",
    "    print(test_data.head(), \"\\n\")\n",
    "\n",
    "    print(\"Missing values in Train dataset:\")\n",
    "    print(train_data.isnull().sum(), \"\\n\")\n",
    "\n",
    "    print(\"Missing values in Validation dataset:\")\n",
    "    print(val_data.isnull().sum(), \"\\n\")\n",
    "\n",
    "    print(\"Missing values in Test dataset:\")\n",
    "    print(test_data.isnull().sum(), \"\\n\")\n",
    "\n",
    "    print(\"Class distribution in Train dataset:\")\n",
    "    print(train_data['Label'].value_counts(), \"\\n\")\n",
    "\n",
    "    print(\"Class distribution in Validation dataset:\")\n",
    "    print(val_data['Label'].value_counts(), \"\\n\")\n",
    "\n",
    "    print(\"Class distribution in Test dataset:\")\n",
    "    print(test_data['Label'].value_counts(), \"\\n\")\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(train_data, val_data, test_data):\n",
    "    le = LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Extract the 'Label' column for each dataset\n",
    "    train_labels = train_data['Label']\n",
    "    val_labels = val_data['Label']\n",
    "    test_labels = test_data['Label']\n",
    "\n",
    "    # Print unique labels before encoding\n",
    "    print(\"Unique labels in train data:\", train_labels.unique())\n",
    "    print(\"Unique labels in validation data:\", val_labels.unique())\n",
    "    print(\"Unique labels in test data:\", test_labels.unique())\n",
    "\n",
    "    print(\"\\nFitting LabelEncoder on training labels...\")\n",
    "    le.fit(train_labels)  # Fit on train data only\n",
    "    print(\"Classes found by LabelEncoder:\", le.classes_)\n",
    "\n",
    "    # Encode the labels for train, validation, and test datasets\n",
    "    print(\"\\nEncoding labels for train, validation, and test datasets...\")\n",
    "    train_labels_encoded = le.transform(train_labels)\n",
    "    val_labels_encoded = le.transform(val_labels)\n",
    "    test_labels_encoded = le.transform(test_labels)\n",
    "\n",
    "    # Print the encoded labels for verification\n",
    "    print(\"\\nEncoded Train Labels (first 10):\", train_labels_encoded[:10])\n",
    "    print(\"Encoded Validation Labels (first 10):\", val_labels_encoded[:10])\n",
    "    print(\"Encoded Test Labels (first 10):\", test_labels_encoded[:10])\n",
    "\n",
    "    # Process features for each dataset (train, val, test)\n",
    "    # --- Train dataset ---\n",
    "    print(\"\\nProcessing features for the train dataset...\")\n",
    "    train_features = train_data.drop(columns=['Label'])\n",
    "    train_features_numeric = train_features.select_dtypes(include=['int64'])\n",
    "    train_features_categorical = train_features.select_dtypes(include=['object'])\n",
    "\n",
    "    print(\"Train features (numeric):\", train_features_numeric.shape)\n",
    "    print(\"Train features (categorical):\", train_features_categorical.shape)\n",
    "\n",
    "    print(\"\\nEncoding categorical features in train data...\")\n",
    "    train_features_categorical_encoded = train_features_categorical.apply(le.fit_transform)\n",
    "    train_features_combined = pd.concat([train_features_numeric, train_features_categorical_encoded], axis=1)\n",
    "\n",
    "    print(\"\\nCombined train features (before scaling):\\n\", train_features_combined.head())\n",
    "\n",
    "    print(\"\\nScaling train features...\")\n",
    "    train_features_scaled = scaler.fit_transform(train_features_combined)\n",
    "    print(\"Scaled train features shape:\", train_features_scaled.shape)\n",
    "\n",
    "    # --- Validation dataset ---\n",
    "    print(\"\\nProcessing features for the validation dataset...\")\n",
    "    val_features = val_data.drop(columns=['Label'])\n",
    "    val_features_numeric = val_features.select_dtypes(include=['int64'])\n",
    "    val_features_categorical = val_features.select_dtypes(include=['object'])\n",
    "\n",
    "    print(\"Validation features (numeric):\", val_features_numeric.shape)\n",
    "    print(\"Validation features (categorical):\", val_features_categorical.shape)\n",
    "\n",
    "    print(\"\\nEncoding categorical features in validation data...\")\n",
    "    val_features_categorical_encoded = val_features_categorical.apply(le.fit_transform)\n",
    "    val_features_combined = pd.concat([val_features_numeric, val_features_categorical_encoded], axis=1)\n",
    "\n",
    "    print(\"\\nCombined validation features (before scaling):\\n\", val_features_combined.head())\n",
    "\n",
    "    print(\"\\nScaling validation features...\")\n",
    "    val_features_scaled = scaler.transform(val_features_combined)\n",
    "    print(\"Scaled validation features shape:\", val_features_scaled.shape)\n",
    "\n",
    "    # --- Test dataset ---\n",
    "    print(\"\\nProcessing features for the test dataset...\")\n",
    "    test_features = test_data.drop(columns=['Label'])\n",
    "    test_features_numeric = test_features.select_dtypes(include=['int64'])\n",
    "    test_features_categorical = test_features.select_dtypes(include=['object'])\n",
    "\n",
    "    print(\"Test features (numeric):\", test_features_numeric.shape)\n",
    "    print(\"Test features (categorical):\", test_features_categorical.shape)\n",
    "\n",
    "    print(\"\\nEncoding categorical features in test data...\")\n",
    "    test_features_categorical_encoded = test_features_categorical.apply(le.fit_transform)\n",
    "    test_features_combined = pd.concat([test_features_numeric, test_features_categorical_encoded], axis=1)\n",
    "\n",
    "    print(\"\\nCombined test features (before scaling):\\n\", test_features_combined.head())\n",
    "\n",
    "    print(\"\\nScaling test features...\")\n",
    "    test_features_scaled = scaler.transform(test_features_combined)\n",
    "    print(\"Scaled test features shape:\", test_features_scaled.shape)\n",
    "\n",
    "    # Prepare final datasets\n",
    "    X_train = train_features_scaled\n",
    "    y_train = train_labels_encoded\n",
    "    X_val = val_features_scaled\n",
    "    y_val = val_labels_encoded\n",
    "    X_test = test_features_scaled\n",
    "    y_test = test_labels_encoded\n",
    "\n",
    "    print(\"\\nFinal shapes of processed data:\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    # Get unique labels across all datasets (train, val, test) and store in a list\n",
    "    labels = list(set(train_labels.unique()).union(val_labels.unique(), test_labels.unique()))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, le, scaler, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_class_distribution(data, column='Label'):\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=column, data=data, palette='Set2')\n",
    "    plt.title(f\"Class Distribution in {column} Column\", fontsize=14)\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.tick_params(axis='both', labelsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "\n",
    "def y_axis_formatter(x, pos):\n",
    "    return f'{x/1000:.1f}'\n",
    "\n",
    "def x_axis_formatter(x, pos):\n",
    "    return f'{x/100000:.1f}'\n",
    "\n",
    "def decimal_formatter(x, pos):\n",
    "    if x == int(x):\n",
    "        return f'{int(x)}'\n",
    "    return f'{x:.2f}'.rstrip('0').rstrip('.')\n",
    "\n",
    "def plot_numeric_feature_distribution(data, numeric_features):\n",
    "    ax = data[numeric_features].hist(bins=20, figsize=(35, 30))\n",
    "    plt.suptitle(\"Distribution of Numeric Features in Dataset\", fontsize=40)\n",
    "    numeric_features_list = numeric_features.tolist()\n",
    "\n",
    "    for i in range(ax.shape[0]):\n",
    "        for j in range(ax.shape[1]):\n",
    "            axes = ax[i, j] if ax.ndim > 1 else ax[j]\n",
    "            feature_index = i * ax.shape[1] + j\n",
    "            if feature_index < len(numeric_features_list):\n",
    "                feature_name = numeric_features_list[feature_index]\n",
    "            else:\n",
    "                continue\n",
    "            axes.set_title(f\"Feature {feature_name}\", fontsize=28)\n",
    "            axes.tick_params(axis='x', labelsize=24)\n",
    "            axes.tick_params(axis='y', labelsize=24)\n",
    "            axes.set_xlabel(\"Feature Value\", fontsize=26)\n",
    "            axes.set_ylabel(\"Frequency\", fontsize=26)\n",
    "\n",
    "            axes.xaxis.set_major_formatter(FuncFormatter(x_axis_formatter))\n",
    "            axes.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n",
    "\n",
    "            x_min, x_max = axes.get_xlim()\n",
    "            y_min, y_max = axes.get_ylim()\n",
    "\n",
    "            x_order = int(np.floor(np.log10(x_max)))\n",
    "            y_order = int(np.floor(np.log10(y_max)))\n",
    "\n",
    "            axes.text(0.95, 0.98, f'x-axis: $10^{x_order}$', transform=axes.transAxes,\n",
    "                      fontsize=26, ha='right', va='top', color='black')\n",
    "            axes.text(0.95, 0.92, f'y-axis: $10^{y_order}$', transform=axes.transAxes,\n",
    "                      fontsize=26, ha='right', va='top', color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.94)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_matrix(data, numeric_features):\n",
    "    correlation_matrix = data[numeric_features].corr()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "    plt.tick_params(axis='x', labelsize=10)\n",
    "    plt.tick_params(axis='y', labelsize=10)\n",
    "    \n",
    "    plt.title(\"Correlation Matrix of Numeric Features\", fontsize=14)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, model_name, optimizer_name, activation_function, lr):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f\"{model_name} - {optimizer_name} - Activation: {activation_function} - LR: {lr}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f\"{model_name} - {optimizer_name} - Activation: {activation_function} - LR: {lr}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_INSTALLED = True\n",
    "except ImportError:\n",
    "    TORCH_INSTALLED = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TF_INSTALLED = True\n",
    "except ImportError:\n",
    "    TF_INSTALLED = False\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    PSUTIL_INSTALLED = True\n",
    "except ImportError:\n",
    "    PSUTIL_INSTALLED = False\n",
    "\n",
    "\n",
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        return __import__(package_name).__version__\n",
    "    except ImportError:\n",
    "        return \"Not Installed\"\n",
    "    except AttributeError:\n",
    "        return \"Unknown Version\"\n",
    "\n",
    "\n",
    "def get_environment_details():\n",
    "    # Core environment details\n",
    "    env_details = {\n",
    "        \"Python Version\": platform.python_version(),\n",
    "        \"OS\": platform.system(),\n",
    "        \"OS Version\": platform.version(),\n",
    "        \"Processor\": platform.processor(),\n",
    "        \"Platform\": platform.platform(),\n",
    "        \"CPU Count\": os.cpu_count(),\n",
    "    }\n",
    "\n",
    "    # GPU information (if available)\n",
    "    if TORCH_INSTALLED and torch.cuda.is_available():\n",
    "        env_details[\"GPU Device\"] = torch.cuda.get_device_name(0)\n",
    "        env_details[\"CUDA Version\"] = torch.version.cuda\n",
    "    elif TF_INSTALLED:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            env_details[\"GPU Device\"] = gpus[0].name\n",
    "            env_details[\"CUDA Version\"] = \"Via TensorFlow\"\n",
    "        else:\n",
    "            env_details[\"GPU Device\"] = \"No GPU Detected\"\n",
    "    else:\n",
    "        env_details[\"GPU Device\"] = \"No GPU Detected\"\n",
    "\n",
    "    # Memory information (if psutil is available)\n",
    "    if PSUTIL_INSTALLED:\n",
    "        mem_info = psutil.virtual_memory()\n",
    "        env_details[\"Total Memory (GB)\"] = round(mem_info.total / (1024 ** 3), 2)\n",
    "    else:\n",
    "        env_details[\"Total Memory (GB)\"] = \"Install psutil for details\"\n",
    "\n",
    "    # Relevant libraries and versions\n",
    "    packages = [\n",
    "        \"numpy\", \"pandas\", \"tensorflow\", \"torch\", \"sklearn\", \n",
    "        \"matplotlib\", \"scipy\", \"seaborn\", \"pillow\", \"imblearn\"\n",
    "    ]\n",
    "    package_versions = {pkg: get_package_version(pkg) for pkg in packages}\n",
    "    \n",
    "    return env_details, package_versions\n",
    "\n",
    "\n",
    "def display_environment_table(env_details, package_versions, save_to_file=True):\n",
    "    # Create DataFrames for better presentation\n",
    "    env_data = pd.DataFrame(\n",
    "        list(env_details.items()), \n",
    "        columns=[\"Component\", \"Version\"]\n",
    "    )\n",
    "\n",
    "    package_data = pd.DataFrame(\n",
    "        list(package_versions.items()), \n",
    "        columns=[\"Library\", \"Version\"]\n",
    "    )\n",
    "\n",
    "    # Print to console\n",
    "    print(\"\\n--- Environment Details ---\")\n",
    "    print(env_data.to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- Installed Packages ---\")\n",
    "    print(package_data.to_markdown(index=False))\n",
    "\n",
    "    if save_to_file:\n",
    "        # Save both to a Markdown file\n",
    "        with open(\"environment_details.md\", \"w\") as f:\n",
    "            f.write(\"# Experiment Environment Details\\n\\n\")\n",
    "            f.write(\"## System Information\\n\")\n",
    "            f.write(env_data.to_markdown(index=False) + \"\\n\\n\")\n",
    "            f.write(\"## Installed Packages\\n\")\n",
    "            f.write(package_data.to_markdown(index=False) + \"\\n\")\n",
    "        print(\"\\nEnvironment details saved to `environment_details.md`.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
